# Module 21: Projet Capstone

## ğŸ¯ Objectif
RÃ©aliser un projet complet de Data Mining de A Ã  Z

---

## 1. Cahier des Charges

### 1.1 Choix du Projet

**Options suggÃ©rÃ©es:**
1. PrÃ©diction de churn clients
2. SystÃ¨me de recommandation
3. DÃ©tection de fraude
4. Analyse de sentiments
5. PrÃ©diction de ventes
6. Classification d'images mÃ©dicales

### 1.2 Livrables

âœ… Code source complet  
âœ… Documentation technique  
âœ… Rapport d'analyse  
âœ… PrÃ©sentation (slides)  
âœ… Dashboard interactif  

---

## 2. MÃ©thodologie

### Phase 1: DÃ©finition (Semaine 1)
- Choisir le problÃ¨me
- DÃ©finir les objectifs
- Identifier les donnÃ©es
- Planifier les Ã©tapes

### Phase 2: Collecte et PrÃ©paration (Semaine 2)
- Collecter les donnÃ©es
- Nettoyage
- Feature engineering
- EDA complÃ¨te

### Phase 3: ModÃ©lisation (Semaine 3)
- SÃ©lectionner algorithmes
- EntraÃ®ner modÃ¨les
- Optimiser hyperparamÃ¨tres
- Ã‰valuer performances

### Phase 4: DÃ©ploiement (Semaine 4)
- CrÃ©er API
- DÃ©velopper dashboard
- Documentation
- PrÃ©sentation

---

## 3. Structure du Projet

```
capstone-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ external/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration.ipynb
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_modeling.ipynb
â”‚   â””â”€â”€ 04_evaluation.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py
â”‚   â””â”€â”€ dashboard.py
â”œâ”€â”€ tests/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ trained_models/
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ final_report.pdf
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ presentation.pptx
```

---

## 4. Exemple: PrÃ©diction de Churn

### 4.1 Data Processing

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

class ChurnPredictor:
    def __init__(self):
        self.scaler = StandardScaler()
        self.model = None
        
    def load_data(self, filepath):
        df = pd.read_csv(filepath)
        return df
    
    def preprocess(self, df):
        # Nettoyage
        df = df.dropna()
        
        # Features
        df['tenure_years'] = df['tenure'] / 12
        df['avg_monthly_charges'] = df['total_charges'] / df['tenure']
        
        # Encoder
        df = pd.get_dummies(df, drop_first=True)
        
        return df
    
    def train(self, X, y):
        from sklearn.ensemble import RandomForestClassifier
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        self.model = RandomForestClassifier(n_estimators=100)
        self.model.fit(X_train_scaled, y_train)
        
        score = self.model.score(self.scaler.transform(X_test), y_test)
        return score
```

### 4.2 API Flask

```python
from flask import Flask, request, jsonify
import joblib
import pandas as pd

app = Flask(__name__)
model = joblib.load('models/churn_model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    df = pd.DataFrame([data])
    prediction = model.predict(df)
    probability = model.predict_proba(df)[0][1]
    
    return jsonify({
        'churn': int(prediction[0]),
        'probability': float(probability)
    })

if __name__ == '__main__':
    app.run(debug=True)
```

### 4.3 Dashboard Streamlit

```python
import streamlit as st
import pandas as pd
import plotly.express as px

st.title("ğŸ“Š Churn Prediction Dashboard")

# Upload data
uploaded_file = st.file_uploader("Upload CSV", type=['csv'])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    
    # MÃ©triques
    col1, col2, col3 = st.columns(3)
    col1.metric("Total Clients", len(df))
    col2.metric("Churn Rate", f"{df['churn'].mean()*100:.1f}%")
    col3.metric("Avg Tenure", f"{df['tenure'].mean():.1f} mois")
    
    # Visualisations
    fig = px.histogram(df, x='tenure', color='churn', title='Distribution Tenure')
    st.plotly_chart(fig)
    
    # PrÃ©diction
    st.header("Faire une PrÃ©diction")
    tenure = st.slider("Tenure (mois)", 0, 100, 12)
    monthly = st.number_input("Charges mensuelles", 0.0, 200.0, 50.0)
    
    if st.button("PrÃ©dire"):
        # Appeler API
        result = make_prediction({'tenure': tenure, 'monthly': monthly})
        st.success(f"ProbabilitÃ© de churn: {result['probability']:.2%}")
```

---

## 5. Ã‰valuation du Projet

### CritÃ¨res (100 points)

**Code et Technique (40 points)**
- QualitÃ© du code: 10 pts
- MÃ©thodologie: 10 pts
- Performances du modÃ¨le: 15 pts
- ReproductibilitÃ©: 5 pts

**Documentation (25 points)**
- README complet: 5 pts
- Code commentÃ©: 10 pts
- Rapport technique: 10 pts

**Analyse et Insights (20 points)**
- EDA approfondie: 10 pts
- InterprÃ©tation rÃ©sultats: 10 pts

**PrÃ©sentation (15 points)**
- ClartÃ©: 5 pts
- Visualisations: 5 pts
- Communication: 5 pts

---

## 6. Conseils

âœ… Commencer simple, itÃ©rer  
âœ… Documenter au fur et Ã  mesure  
âœ… Versionner avec Git  
âœ… Tester le code  
âœ… Demander feedback rÃ©guliÃ¨rement  
âœ… GÃ©rer le temps efficacement  

---

## ğŸ† FÃ©licitations!

Vous avez complÃ©tÃ© la formation Data Mining!

Vous Ãªtes maintenant capable de:
- âœ… Programmer en Python, Java, Julia/Spark
- âœ… GÃ©rer des bases de donnÃ©es
- âœ… Utiliser le cloud
- âœ… Appliquer des techniques de Data Mining
- âœ… Construire et dÃ©ployer des modÃ¨les ML

**Prochaines Ã©tapes:**
1. Construire votre portfolio
2. Contribuer Ã  des projets open source
3. Participer Ã  des compÃ©titions Kaggle
4. Chercher des opportunitÃ©s professionnelles

**Bonne chance dans votre carriÃ¨re en Data Science! ğŸš€**

---

*Â© 2024 - Formation Data Mining Professionnelle*
